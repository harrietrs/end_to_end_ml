{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End-to-end Machine Learning**\n",
    "\n",
    "Borrows very heavily from _Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow_ by Aurelien Geron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "# Display all columns\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"image_outputs\")\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARTHQUAKE_PATH = os.path.join(\"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path=EARTHQUAKE_PATH, file_name = \"train_values.csv\"):\n",
    "    csv_path = os.path.join(data_path, file_name)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_values = load_data()\n",
    "raw_labels = load_data(file_name=\"train_labels.csv\")\n",
    "raw_data = pd.merge(raw_labels,raw_values,on='building_id')\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a quick look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 260,601 instances in the dataset, if you want to do a quick explore, I'd recommend using a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.sample(frac=0.5)\n",
    "raw_data.reset_index(drop=True,inplace=True)\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at the top five rows using the DataFrame’s head() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents a building. \n",
    "\n",
    "The `damage_grade` field is what we want to be able to predict, our _label_. There are 38 attributes, or _features_ relating to geographic area, house characteristics: age, type of roof, use, etc.\n",
    "\n",
    "Let's take a closer look at these attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! There are no null values. Watch out, as this might be because missing values have not been encoded correctly (e.g. entered as 'missing' or blanks). \n",
    "\n",
    "In our data, most attributes seem to be numerical, but on closer inspection, the fields such as `has_secondary_use_other` are essentially boolean values. They have been already converted from objects, using a method called 'one hot encoding' which I will go into later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_encoded_features = [s for s in raw_data.columns if 'has_' in s]\n",
    "raw_data[pre_encoded_features] = raw_data[pre_encoded_features].astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some fields, such as the fields ending in `_type` are objects. This 'object' type could mean a variety of types, such as a string, or text attribute. Here, they refer to distinct categories. You can find out what categories exist and how many buildings belong to each category by using the `value_counts()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[\"roof_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.drop('building_id',axis=1).select_dtypes(include=('int64')).hist(bins=50, figsize=(20,15))\n",
    "plt.show()\n",
    "save_fig(\"attribute_histogram_plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = list(raw_data.select_dtypes(include=['object']).columns)\n",
    "numerical_features = ['area_percentage','height_percentage']\n",
    "discrete_features = list(raw_data.drop(columns=categorical_features+numerical_features+pre_encoded_features+['building_id']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical features, `countplot()` can be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(20, 10))\n",
    "for variable, subplot in zip(categorical_features, ax.flatten()):\n",
    "    sns.countplot(raw_data[variable], ax=subplot)\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "plt.show()\n",
    "save_fig(\"cat_attribute_countplots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a closer look at our _label_: `damage_grade`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['damage_grade'] = raw_data['damage_grade'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.countplot(raw_data['damage_grade'], ax=ax)\n",
    "plt.show()\n",
    "save_fig(\"class_balance_countplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.damage_grade.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.damage_grade.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is imbalanced with 33% ‘high’ damage grade,10% of ‘Low’ damage grade and 57% of ‘Medium’ damage grade. These imbalances can be adjusted at the modelling stage, using `class_weight`, but for clarity I am manually adjusting the class imbalance by taking an equal amount randomly from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_high=raw_data[raw_data.damage_grade==3]\n",
    "train_medium=raw_data[raw_data.damage_grade==2]\n",
    "train_low=raw_data[raw_data.damage_grade==1]\n",
    "train_high=train_high.sample(25000)\n",
    "train_medium=train_medium.sample(25000)\n",
    "train_low=train_low.sample(25000)\n",
    "resampled_data = pd.concat([train_high, train_medium,train_low], ignore_index=True, sort =False)\n",
    "resampled_data=resampled_data.sample(frac=1).reset_index(drop=True)\n",
    "training_data = resampled_data.copy()\n",
    "training_data = raw_data.copy()\n",
    "resampled_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the `age` feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.age.hist(bins=50, figsize=(10,5))\n",
    "plt.title('Building Age Histogram (Before)')\n",
    "save_fig(\"age_hist_plot_pre_cleaning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on >200?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_age_greater_200 = training_data[training_data.age>200]\n",
    "sns.countplot(x ='age', data= training_data_age_greater_200, hue='damage_grade')\n",
    "save_fig('age_outliers_countplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh, looks like we have some outliers. This seems like something we can exclude, as it is unlikely 1400 buildings are exactly 999 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_index_to_drop = training_data[training_data['age'] > 200 ].index\n",
    " \n",
    "# Delete these row indexes from dataFrame\n",
    "training_data.drop(age_index_to_drop , inplace=True)\n",
    "training_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would add this step to our processing function as part of our pipeline, which I will show you late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.age.hist()\n",
    "save_fig('age_hist_plot_post_cleaning')\n",
    "plt.title('Building Age Histogram (After)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how damage varies with age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "plt.subplot(211)\n",
    "plt.title('Damage snapshot properties with less than 50 years old')\n",
    "tr=training_data[training_data['age']<=50]\n",
    "sns.countplot(x='age',data=tr,hue='damage_grade')\n",
    "save_fig('age_countplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this notebook, I am going to drop `building_id` at this stage, using the index as identifier. In a real project, you may need to retain these for use later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_ids = training_data.pop('building_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your brain is an amazing pattern detection system, which means that it is highly prone to overfitting: if you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of Machine Learning model. When you estimate the generalization error using the test set, your estimate will be too optimistic, and you will launch a system that will not perform as well as expected. This is called _data snooping_ bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is `train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(training_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random sampling methods are generally fine if your dataset is large enough, but if it is not, you run the risk of introducing a significant sampling bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified sampling for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You chat with experts, who say that the age of the building is an important feature to predict damage. You may want to ensure that the test set is representative of the various ages in the whole dataset. Since the age income is a discrete numerical attribute, you first need to create an age category attribute. \n",
    "\n",
    "Most age values are clustered around 0-25 but some are much older. We need a sufficient number of instances in your dataset for each stratum, or the estimate of a stratum’s importance may be biased. We should not have too many strata, and each stratum should be large enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"age_cat\"] = pd.cut(training_data[\"age\"],\n",
    "                               bins=[-np.inf, 5, 15,25, 40, np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"age_cat\"].hist()\n",
    "plt.title('Building age grouped into buckets')\n",
    "save_fig('age_bucket_histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(training_data, training_data[\"age_cat\"]):\n",
    "    strat_train_set = training_data.loc[train_index]\n",
    "    strat_test_set = training_data.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"age_cat\"].value_counts(ascending=True) / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"age_cat\"].value_counts(ascending=True) / len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_cat_proportions(data):\n",
    "    return data[\"age_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(training_data, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": age_cat_proportions(training_data),\n",
    "    \"Stratified\": age_cat_proportions(strat_test_set),\n",
    "    \"Random\": age_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the incremental gains are very small, as my dataset is so large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"age_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover and visualize the data to gain insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake = train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I compute the standard correlation coefficient between every pair of attributes using the `corr()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = earthquake.corr()\n",
    "g = sns.heatmap(corr_matrix, center=0,\n",
    "            square=True, linewidths=0.01, cbar_kws={\"shrink\": .5}, annot=False, yticklabels=True,\n",
    "                xticklabels=True,cmap=sns.diverging_palette(20, 220, n=200))\n",
    "sns.despine()\n",
    "g.figure.set_size_inches(20,10)\n",
    "save_fig('correlation_matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature `has_superstructure_mud_mortar_stone` seems to be the feature most positively correlated with our label, `damage_grade`. Let's explore this some more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"count_floors_pre_eq\", y=\"age\", col=\"has_superstructure_mud_mortar_stone\", hue=\"damage_grade\",data=earthquake)\n",
    "save_fig('mud_mortar_stone_catplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing you may want to do before preparing the data for Machine Learning algorithms is to try out various attribute combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake[\"families_per_floor\"] = earthquake[\"count_families\"]/earthquake[\"count_floors_pre_eq\"]\n",
    "earthquake[\"floors_per_area\"] = earthquake[\"count_floors_pre_eq\"]/earthquake[\"area_percentage\"]\n",
    "earthquake[\"height_per_area\"]=earthquake[\"height_percentage\"]/earthquake[\"area_percentage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = earthquake.corr()\n",
    "corr_matrix[\"damage_grade\"].sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. But this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing this manually, you should write functions for this purpose, for several good reasons:\n",
    "\n",
    "- This will allow you to reproduce these transformations easily on any dataset (e.g., the next time you get a fresh dataset).\n",
    "\n",
    "- You will gradually build a library of transformation functions that you can reuse in future projects.\n",
    "\n",
    "- This will make it possible for you to easily try various transformations and see which combination of transformations works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake = train_set.drop([\"damage_grade\"], axis=1) # drop labels for training set\n",
    "earthquake_labels = train_set[\"damage_grade\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. We are lucky enough to be working with a pre-cleaned dataset, with no missing values, but this is very unusual, and we could get some new data that has missing values. You normally have three options:\n",
    "\n",
    "1. Get rid of the corresponding districts.\n",
    "\n",
    "2. Get rid of the whole attribute.\n",
    "\n",
    "3. Set the values to some value (zero, the mean, the median, etc.).\n",
    "\n",
    "You can accomplish these easily using DataFrame’s `dropna()`, `drop()`, and `fillna()` methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn provides a handy class to take care of missing values: `SimpleImputer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the median can only be computed on numerical attributes, you need to create a copy of the data without categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_num = earthquake.drop(categorical_features, axis=1)\n",
    "imputer.fit(earthquake_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use this “trained” imputer to transform the training set by replacing missing values with the learned medians:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(earthquake_num)\n",
    "\n",
    "earthquake_tr = pd.DataFrame(X, columns=earthquake_num.columns,\n",
    "                          index=earthquake_num.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Text and Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only dealt with numerical attributes, but now let’s look at text attributes. In this dataset, there are a few: `land_surface_condition`, `foundation_type`, `roof_type`, `ground_floor_type`, `other_floor_type`, `position`, `plan_configuration` and `legal_ownership_status`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_cat = earthquake[categorical_features]\n",
    "earthquake_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could convert these features to numbers, but one issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad,” “average,” “good,” and “excellent”).\n",
    "\n",
    "Instead, let's create one binary attribute per category. This is called one-hot encoding, because only one attribute will be equal to 1. The new attributes are sometimes called _dummy_ attributes. Scikit-Learn provides a `OneHotEncoder` class to convert categorical values into one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder(sparse=False)\n",
    "earthquake_cat_1hot = cat_encoder.fit_transform(earthquake_cat)\n",
    "earthquake_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes. \n",
    "\n",
    "You will want your transformer to work seamlessly with Scikit-Learn functionalities (such as pipelines).\n",
    "\n",
    "For example, here is a small transformer class that adds the combined attributes we discussed earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def add_extra_features(X):\n",
    "\n",
    "    families_per_floor = X[:, families_ix] / X[:, floors_ix]\n",
    "    floors_per_area = X[:, floors_ix] / X[:, area_ix]\n",
    "    height_per_area = X[:, height_ix] / X[:, area_ix]\n",
    "    return np.c_[X, families_per_floor, floors_per_area,\n",
    "                     height_per_area]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important transformations you need to apply to your data is feature scaling. Most Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two common ways to get all attributes to have the same scale: \n",
    "- min-max scaling\n",
    "- standardization\n",
    "\n",
    "Min-max scaling (many people call this normalization) is the simplest: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min. \n",
    "\n",
    "Standardization first subtracts the mean value and then divides by the standard deviation. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with such sequences of transformations. Here is a small pipeline for the numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "earthquake_num = earthquake.select_dtypes(['int8','int64'])\n",
    "families_ix, floors_ix, area_ix, height_ix = [\n",
    "    list(earthquake_num.columns).index(col)\n",
    "    for col in (\"count_families\", \"count_floors_pre_eq\", \"area_percentage\", \"height_percentage\")]\n",
    "earthquake_num_tr = num_pipeline.fit_transform(earthquake_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column. `ColumnTransformer` works great for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(earthquake_num.columns)\n",
    "cat_attribs = categorical_features\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "earthquake_prepared = full_pipeline.fit_transform(earthquake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select and train a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(earthquake_prepared, earthquake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try the full preprocessing pipeline on a few training instances\n",
    "some_data = earthquake.iloc[:5]\n",
    "some_labels = earthquake_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "print(\"Predictions:\", log_reg.predict(some_data_prepared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare against the actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "earthquake_predictions = log_reg.predict(earthquake_prepared)\n",
    "log_f1 = f1_score(earthquake_labels, earthquake_predictions, average='micro')\n",
    "log_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough. As we saw in the previous chapter, the main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. This model is not regularized, which rules out the last option. You could try to add more features (e.g., the log of the population), but first let’s try a more complex model to see how it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s train a `DecisionTreeRegressor`. This is a powerful model, capable of finding complex nonlinear relationships in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(earthquake_prepared, earthquake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_predictions = tree_clf.predict(earthquake_prepared)\n",
    "tree_f1 = f1_score(earthquake_labels, earthquake_predictions,average='micro')\n",
    "tree_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what!? Could this model really be almost absolutely perfect? Of course, it is much more likely that the model has badly overfit the data. How can you be sure? As we saw earlier, you don’t want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training and part of it for model validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better evaluation using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On option is Scikit-Learn’s K-fold cross-validation feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tree_f1_scores = cross_val_score(tree_clf, earthquake_prepared, earthquake_labels,\n",
    "                         scoring=\"f1_micro\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "display_scores(tree_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Decision Tree doesn’t look as good as it did earlier. \n",
    "\n",
    "Let’s compute the same scores for the Logistic Regression model just to be sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scores = cross_val_score(log_reg, earthquake_prepared, earthquake_labels,\n",
    "                             scoring=\"f1_micro\", cv=10)\n",
    "display_scores(log_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try one last model now: the RandomForestRegressor. As we will see in Chapter 7, Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning, and it is often a great way to push ML algorithms even further. We will skip most of the code since it is essentially the same as for the other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "forest_clf.fit(earthquake_prepared, earthquake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_scores = cross_val_score(forest_clf, earthquake_prepared, earthquake_labels,\n",
    "                                scoring=\"f1_micro\", cv=10)\n",
    "display_scores(forest_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn’s `GridSearchCV` searches among the hyperparameters you want it to experiment with, using values you choose. It will use cross-validation to evaluate all the possible combinations of hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3×3) combinations of hyperparameters\n",
    "    {'n_estimators': [30, 50, 100], 'max_features': [6,8,10]}\n",
    "  ]\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "# train across 5 folds, that's a total of (9)*5=45 rounds of training \n",
    "grid_search = GridSearchCV(forest_clf, param_grid, cv=5,\n",
    "                           scoring='f1_micro', return_train_score=True)\n",
    "grid_search.fit(earthquake_prepared, earthquake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the score of each hyperparameter combination tested during the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Best Models and Their Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will often gain good insights on the problem by inspecting the best models. For example, the `RandomForestClassifier` can indicate the relative importance of each attribute for making accurate predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"families_per_floor\", \"floors_per_area\", \"height_per_area\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Your System on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set. There is nothing special about this process; just get the predictors and the labels from your test set, run your full_pipeline to transform the data and evaluate the final model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = test_set.drop([\"damage_grade\"], axis=1)\n",
    "y_test = test_set[\"damage_grade\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_f1 = f1_score(y_test, final_predictions, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A full pipeline with both preparation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", full_pipeline),\n",
    "        (\"random_forest\", final_model)\n",
    "])\n",
    "\n",
    "full_pipeline_with_predictor.fit(earthquake, earthquake_labels)\n",
    "print(\"Predictions:\", full_pipeline_with_predictor.predict(some_data))\n",
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model persistence using joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = full_pipeline_with_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(my_model, \"my_model.pkl\") \n",
    "my_model_loaded = joblib.load(\"my_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible additions:\n",
    "- Transformation to select k best features\n",
    "- More models (+ ensemble models)\n",
    "- RandomisedSearchCV\n",
    "- GridSearch to find optimum imputation strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use RandomizedSearchCV instead. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration. This approach has two main benefits:\n",
    "\n",
    "If you let the randomized search run for, say, 1,000 iterations, this approach will explore 1,000 different values for each hyperparameter (instead of just a few values per hyperparameter with the grid search approach)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "End to End ML",
   "language": "python",
   "name": "end-to-end-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
